{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HERA: Official Documentation","text":"<p>Welcome to the official documentation for the HERA (Hierarchical-Compensatory, Effect-Size-Driven and Non-Parametric Ranking Algorithm) MATLAB toolbox.</p> <p>This platform serves as the comprehensive reference guide for the HERA framework, systematically organizing the repository's documentation into a centralized, wiki-style knowledge hub.</p> <p>While the source code contains the raw files, this site is optimized to offer a streamlined reading experience for the entire framework\u2014including detailed installation guides, parameter configuration, hierarchical-compensatory ranking logic, and bootstrap validation mechanisms.</p>"},{"location":"#get-hera-v121","title":"Get HERA (v1.2.1)","text":"<p>You can download the latest version of the toolbox directly:</p> <ul> <li>GitHub Releases: Download .mltbx or Source</li> <li>MATLAB File Exchange: Download Toolbox</li> </ul>"},{"location":"Advanced_Usage/","title":"Advanced Usage (Developer Mode)","text":"<p>Developers can call <code>HERA.run_ranking</code> directly with data matrices, bypassing file I/O. This is useful for integration into other pipelines or simulation studies.</p>"},{"location":"Advanced_Usage/#syntax","title":"Syntax","text":"<pre><code>results = HERA.run_ranking(userInput);\n</code></pre>"},{"location":"Advanced_Usage/#input-structure-userinput","title":"Input Structure (<code>userInput</code>)","text":"<p>Instead of <code>folderPath</code>, provide <code>custom_data</code>:</p> <pre><code>% 1. Prepare Data\n% Cell array of (n_Subjects x N_Methods) matrices\ndata_m1 = randn(50, 5); \ndata_m2 = randn(50, 5);\ncustom_data = {data_m1, data_m2};\n\n% 2. Configure User Input\nuserInput = struct();\nuserInput.custom_data = custom_data;\nuserInput.metric_names = {'Accuracy', 'Runtime'}; % Must match data count\nuserInput.dataset_names = {'Method A', 'Method B', 'Method C', 'Method D',\n    'Method E'};\nuserInput.ranking_mode = 'M1_M2';\nuserInput.output_dir = pwd;\n\n% 3. Run\nresults = HERA.run_ranking(userInput);\n</code></pre>"},{"location":"Advanced_Usage/#output-results-struct","title":"Output (<code>results</code> struct)","text":"<ul> <li><code>.final_rank</code>: Final ranking vector.</li> <li><code>.d_vals_all</code>: Effect sizes (Cliff's Delta).</li> <li><code>.p_vals_all</code>: Raw p-values.</li> <li><code>.ci_d_all</code>: Confidence intervals.</li> </ul> <p>For the full reference of the <code>results</code> struct, see Results Structure Reference.</p>"},{"location":"Automated_Build/","title":"Automated Build (GitHub Actions)","text":"<p>Note: The automated build workflow requires a valid MATLAB license to be configured as a secret(<code>MATLAB_LICENSE</code>) in the repository settings. I sadly can not provide a license for this respository. Student licenses may not support this feature.</p> <p>To enable GitHub Actions for building and testing, you need to provide a valid MATLAB license:</p> <ol> <li>Go to your repository's Settings &gt; Secrets and variables &gt; Actions.</li> <li>Click New repository secret.</li> <li>Name the secret <code>MATLAB_LICENSE</code> and paste the contents of your license file.</li> </ol>"},{"location":"Automated_Build/#running-the-build","title":"Running the Build","text":"<p>This workflow is set to manual execution (<code>workflow_dispatch</code>) to save resources. It is currently configured not to trigger automatically on new tags because I, the repository owner, do not have a MATLAB license for the GitHub runner.</p> <p>Note for Forks: If you have a valid MATLAB license configured, you can re-enable automatic builds by uncommenting the <code>release</code> trigger in <code>.github/workflows/build_release.yml</code>.</p> <ol> <li>Navigate to the Actions tab in the repository.</li> <li>Select Build HERA Runtime from the sidebar.</li> <li>Click the Run workflow button.</li> </ol> <p>The workflow performs the following steps:</p> <ol> <li>Unit Testing: Runs the full test suite (<code>HERA.run_unit_test</code>) to ensure    code integrity.</li> <li>Compilation: Builds the standalone application for the target operating    system (macOS/Linux/Windows) using the <code>deploy/build_HERA_matlab.m</code> script.</li> <li>Toolbox Packaging: Packages the code as a MATLAB Toolbox (<code>.mltbx</code>) using <code>package_HERA_toolbox.m</code>.</li> <li>Python Build: Compiles the Python interface using <code>build_HERA_python.m</code>.</li> <li>Artifact Upload: Uploads the compiled installer, toolbox, and Python    package as build artifacts, which can be downloaded from the GitHub Actions    run page.</li> </ol>"},{"location":"Automated_Build/#publishing-to-pypi","title":"Publishing to PyPI","text":"<p>To publish the Python package to PyPI, follow this manual workflow (since the GitHub Runner cannot build the package due to licensing):</p>"},{"location":"Automated_Build/#prerequisites","title":"Prerequisites","text":"<ol> <li>PyPI Trusted Publishing: Configure GitHub Actions as a trusted publisher in your PyPI account settings.</li> <li>Owner: <code>lerdmann1601</code></li> <li>Repository: <code>HERA-Matlab</code></li> <li>Workflow name: <code>publish_pypi.yml</code></li> <li>Environment: <code>pypi</code></li> <li>Local Environment: Ensure you have MATLAB and <code>python3</code>, <code>pip3</code> installed locally.</li> </ol>"},{"location":"Automated_Build/#release-steps","title":"Release Steps","text":"<ol> <li>Prepare Artifacts:    Run the helper script locally to build the package and generate the distribution files:</li> </ol> <pre><code>./deploy/build_and_prep_pypi.sh\n</code></pre> <p>This will create a <code>deploy/dist</code> folder containing <code>.whl</code> and <code>.tar.gz</code> files.</p> <ol> <li>Create Release:</li> <li>Go to GitHub -&gt; Releases -&gt; Draft a new release.</li> <li>Choose a text tag (e.g., <code>v1.2.1</code>).</li> <li>Upload the files from <code>deploy/dist/</code> to the release.</li> <li> <p>Publish the release.</p> </li> <li> <p>Publish:</p> </li> <li>Go to the Actions tab in GitHub.</li> <li>Select Publish to PyPI.</li> <li>Click Run workflow.</li> <li>The action will download the files from your release, patch them with the correct version/metadata, and upload them to PyPI.</li> <li>Recommended: After the action completes, download the final artifacts from PyPI (or the Action run) and replace the manually uploaded files in your GitHub Release. This ensures the Release artifacts match the PyPI version exactly (including the correct version number and metadata).</li> </ol>"},{"location":"Bootstrap_Configuration/","title":"Bootstrap Configuration (Auto-Convergence)","text":"<p>The <code>bootstrap_*</code> structs (e.g., <code>bootstrap_thresholds</code>) support the following nested parameters to control the convergence algorithm:</p> Field Type Default (Thr/CI/Rank) Description <code>B_start</code> int <code>100</code> / <code>100</code> / <code>50</code> Initial number of bootstrap iterations. <code>B_step</code> int <code>100</code> / <code>200</code> / <code>25</code> Iterations to add in each step. <code>B_end</code> int <code>10000</code> / <code>20000</code> / <code>2500</code> Maximum number of iterations. <code>n_trials</code> int <code>25</code> / <code>30</code> / <code>15</code> Number of independent trials per step to check stability. <code>convergence_tolerance</code> double <code>0.01</code> / <code>0.03</code> / <code>0.005</code> Max allowed variation (e.g., 0.005 = 0.5%). <code>smoothing_window</code> int <code>3</code> / <code>3</code> / <code>3</code> Window size for moving average smoothing. <code>convergence_streak_needed</code> int <code>3</code> / <code>3</code> / <code>3</code> Consecutive steps required to pass tolerance. <code>min_steps_for_convergence_check</code> int <code>1</code> Minimum steps before checking convergence. <p>\ud83d\udc49 Convergence Modes and Troubleshooting</p>"},{"location":"Configuration_%26_Parameters/","title":"Configuration Referenc","text":"<p>To run HERA in Batch Mode, create a <code>.json</code> file (e.g., <code>analysis_config.json</code>). Below is the complete list of available parameters.</p>"},{"location":"Configuration_%26_Parameters/#minimal-configuration","title":"Minimal Configuration","text":"<pre><code>{\n  \"userInput\": {\n    \"folderPath\": \"/Users/Name/Data\",\n    \"fileType\": \".csv\",\n    \"metric_names\": [\"Accuracy\", \"Runtime\", \"Memory\"],\n    \"ranking_mode\": \"M1_M2_M3\",\n    \"output_dir\": \"/Users/Name/Results\"\n  }\n}\n</code></pre>"},{"location":"Configuration_%26_Parameters/#full-configuration-example-json","title":"Full Configuration Example (JSON)","text":"<p>This example shows all possible parameters with their default values. Parameters inside <code>system</code> and <code>bootstrap_*</code> must be nested correctly as shown.</p> <pre><code>{\n  \"userInput\": {\n    \"folderPath\": \"/Path/To/Data\",\n    \"fileType\": \".csv\",\n    \"metric_names\": [\"Metric1\", \"Metric2\", \"Metric3\"],\n    \"output_dir\": \"/Path/To/Results\",\n    \"language\": \"en\",\n    \"ranking_mode\": \"M1_M2_M3\",\n    \"reproducible\": true,\n    \"seed\": 123,\n    \"num_workers\": \"auto\",\n    \"create_reports\": true,\n    \"plot_theme\": \"light\",\n    \"ci_level\": 0.95,\n    \"alphas\": [0.05, 0.05, 0.05],\n    \"run_sensitivity_analysis\": true,\n    \"run_power_analysis\": true,\n    \"power_simulations\": 10000,\n    \"min_data_completeness\": 0.80,\n\n    \"system\": {\n      \"target_memory\": \"auto\",\n      \"jack_parfor_thr\": 300,\n      \"jack_vec_limit\": 150,\n      \"delta_mat_limit\": 30000,\n      \"min_batch_size\": 100\n    },\n\n    \"manual_B_thr\": 2000,\n    \"manual_B_ci\": 10000,\n    \"manual_B_rank\": 500,\n\n    \"bootstrap_seed_offset\": 1000,\n\n    \"bootstrap_thresholds\": {\n      \"B_start\": 100,\n      \"B_step\": 100,\n      \"B_end\": 10000,\n      \"n_trials\": 25,\n      \"convergence_tolerance\": 0.01,\n      \"smoothing_window\": 3,\n      \"convergence_streak_needed\": 3,\n      \"min_steps_for_convergence_check\": 1\n    },\n\n    \"bootstrap_ci\": {\n      \"B_start\": 100,\n      \"B_step\": 200,\n      \"B_end\": 20000,\n      \"n_trials\": 30,\n      \"convergence_tolerance\": 0.03,\n      \"smoothing_window\": 3,\n      \"convergence_streak_needed\": 3,\n      \"min_steps_for_convergence_check\": 1\n    },\n\n    \"bootstrap_ranks\": {\n      \"B_start\": 50,\n      \"B_step\": 25,\n      \"B_end\": 2500,\n      \"n_trials\": 15,\n      \"convergence_tolerance\": 0.005,\n      \"smoothing_window\": 3,\n      \"convergence_streak_needed\": 3,\n      \"min_steps_for_convergence_check\": 1\n    }\n  }\n}\n</code></pre>"},{"location":"Configuration_%26_Parameters/#parameter-dictionary","title":"Parameter Dictionary","text":"Category Parameter Type Default Description Input/Output <code>folderPath</code> string - Absolute path to data folder. <code>fileType</code> string - <code>.csv</code> or <code>.xlsx</code>. <code>metric_names</code> array - List of filenames (metrics) in hierarchical order. <code>output_dir</code> string - Path to save results. <code>language</code> string <code>\"en\"</code> Output language code. Logic <code>ranking_mode</code> string <code>\"M1_M2_M3\"</code> Logic mode (<code>M1</code>, <code>M1_M2</code>, <code>M1_M3A</code>, <code>M1_M2_M3</code>). <code>run_sensitivity_analysis</code> bool <code>true</code> Run ranking for all metric permutations. <code>run_power_analysis</code> bool <code>true</code> Run post-hoc power analysis. <code>min_data_completeness</code> double <code>0.80</code> Min fraction of valid pairs required (0.8 = 80%). Statistics <code>ci_level</code> double <code>0.95</code> Confidence interval level (e.g., 0.95 for 95%). <code>alphas</code> array <code>[0.05, ...]</code> Significance level for each metric. <code>power_simulations</code> int <code>10000</code> Number of simulations for power analysis. System <code>reproducible</code> bool <code>true</code> Use fixed RNG seed. <code>seed</code> int <code>123</code> RNG seed value. <code>bootstrap_seed_offset</code> int <code>1000</code> Base random seed offset for parallel bootstrap operations. Prevents RNG substream collisions. <code>num_workers</code> int/str <code>\"auto\"</code> Number of parallel workers. <code>\"auto\"</code> uses <code>parcluster('local').NumWorkers</code>. <code>system.target_memory</code> int <code>\"auto\"</code> Target memory per chunk (MB). Automatically calculated based on available RAM, but can be manually defined via JSON config file. <code>system.jack_parfor_thr</code> int <code>300</code> Min n to trigger parallel execution. <code>system.jack_vec_limit</code> int <code>150</code> Max n for vectorized Jackknife calculations. <code>system.delta_mat_limit</code> int <code>30000</code> Max n_x * n_y product (usually n^2) for matrix-based Cliff's Delta. <code>system.min_batch_size</code> int <code>100</code> Min batch size for parallel processing. Graphics <code>create_reports</code> bool <code>true</code> Generate PDF reports and high-res plots. If <code>false</code>, only essential convergence and diagnostics plots are saved. <code>plot_theme</code> string <code>\"light\"</code> <code>\"light\"</code>, <code>\"dark\"</code>, <code>\"colourblind light\"</code>, or <code>\"colourblind dark\"</code>. Bootstrap (Manual) <code>manual_B_thr</code> int <code>2000</code> Iterations for Thresholds (empty = auto). <code>manual_B_ci</code> int <code>5000</code> Iterations for CIs (empty = auto). <code>manual_B_rank</code> int <code>500</code> Iterations for Rank Stability (empty = auto). Bootstrap (Auto) <code>bootstrap_thresholds</code> struct (See Below) Config for Threshold convergence. <code>bootstrap_ci</code> struct (See Below) Config for CI convergence. <code>bootstrap_ranks</code> struct (See Below) Config for Rank Stability convergence. <p>\ud83d\udc49 Bootstrap Configuration (Auto-Convergence)</p>"},{"location":"Convergence_Analysis/","title":"Convergence Analysis for Robust Mode","text":"<p>This document provides an overview of the validation performed for the Robust Convergence Mode in HERA.</p>"},{"location":"Convergence_Analysis/#overview","title":"Overview","text":"<p>The purpose of this analysis was to ensure that the default parameters used in the robust convergence mode are effective across a variety of data scenarios. Rather than a formal theoretical study, this was a practical validation using Monte Carlo simulations to verify that convergence is reliably achieved within the default iteration limits across typical use cases.</p> <p>Additionally, we evaluated the accuracy of the results by comparing them against reference values generated with a very high number of bootstrap iterations. This ensures that the convergence criteria not only stop at a reasonable time but also produce results with acceptable error relative to the reference values.</p> <p>The results confirm that the default settings are appropriate for the vast majority of scenarios, yielding both stable and accurate outcomes.</p> <p>To ensure reproducibility and fair comparability between the Robust Mode and the reference runs, we utilized a fixed random seed for all simulations. This guarantees that both the convergence checks and the reference benchmarks were performed on identical data sequences and bootstrap samples.</p>"},{"location":"Convergence_Analysis/#analysis-design-constraints","title":"Analysis Design &amp; Constraints","text":"<p>To validate the robustness of the default parameters, we simulated a diverse set of conditions.</p>"},{"location":"Convergence_Analysis/#fixed-parameters","title":"Fixed Parameters","text":"<p>It is important to note that the following parameters were treated as constants and thus their influence was not explicitly tested in this analysis:</p> <ul> <li>Step Size (\\(B_{step}\\))</li> <li>Tolerance (\\(Tol\\))</li> <li>Start Iterations (\\(B_{start}\\))</li> <li>End Iterations (\\(B_{end}\\))</li> </ul> <p>These were held constant because the three parameters we did test (regarding data distribution and method variability) were considered the most critical factors influencing the stability of achieving a desired tolerance.</p>"},{"location":"Convergence_Analysis/#evaluated-scenarios","title":"Evaluated Scenarios","text":"<p>We tested against multiple distribution types (Normal, Bimodal, Skewed, Likert) and sample sizes, as well as varying method properties.</p> <p>Data Scenarios: </p> <p>Method Configurations: </p>"},{"location":"Convergence_Analysis/#analysis-scope","title":"Analysis Scope","text":"<p>To isolate the behavior of individual convergence algorithms, the following simplifications were applied:</p> <ul> <li>Effect Size Metrics: Both Cliff's Delta and Relative Difference are always calculated, and the convergence criterion is based on the averaged stability of both. Only Cliff's Delta was used as the accuracy metric for comparison against reference values. This design choice allows us to test whether the averaged convergence approach produces accurate individual estimates despite relying on a combined stability indicator.</li> <li>Ranking Stability: A single-metric ranking (M1 only) was used to provide a stable reference point. Multi-metric rankings (M1_M2, M1_M2_M3) may exhibit different convergence characteristics due to increased complexity.</li> <li>Thresholds for Ranking: Pre-calculated reference thresholds were used to isolate the ranking convergence behavior from threshold estimation variability.</li> </ul>"},{"location":"Convergence_Analysis/#validation-results","title":"Validation Results","text":"<p>We evaluated the performance of the Robust Mode based on two key criteria: Convergence Success (did it finish?) and Accuracy (were the results correct?).</p>"},{"location":"Convergence_Analysis/#1-convergence-frequency","title":"1. Convergence Frequency","text":"<p>In our analysis, we observed that:</p> <ul> <li>The Robust Mode achieved exceptionally high convergence rates.</li> <li>The Ranking phase was the only area where non-convergence was observed, and even then, it occurred in &lt; 0.3% of ranking cases.</li> <li>This implies that for all other metrics (BCa Intervals, Thresholds), convergence was virtually 100%.</li> <li>In the rare ranking failures, the maximum number of iterations (<code>B_end</code>) was reached before the convergence criterion was satisfied. To resolve this we increased <code>B_end</code> for the default settings provided.</li> </ul>"},{"location":"Convergence_Analysis/#2-accuracy-validation","title":"2. Accuracy Validation","text":"<p>Beyond simply checking for convergence, we also validated the accuracy of the results.</p> <ul> <li>We compared the outcomes of the Robust Mode against \"Gold Standard\" reference values generated with very high bootstrap iteration counts:</li> <li>Thresholds: \\(B_{ref} = 15{,}000\\)</li> <li>BCa Confidence Intervals: \\(B_{ref} = 30{,}000\\)</li> <li>Ranking Stability: \\(B_{ref} = 5{,}000\\)</li> <li>The goal was to analyze the distribution of errors (deviation from reference) to ensure that the convergence method yields results that are not just stable, but also practically in line with the theoretical limits.</li> <li>The results confirmed that the Robust Mode produces estimates with acceptably small deviation from reference values across all simulated scenarios. Median absolute errors were typically well below 5% relative to the reference values. This also validates the averaged convergence approach: despite determining convergence from a combined stability indicator, the individual accuracy of Cliff's Delta remained excellent.</li> </ul>"},{"location":"Convergence_Analysis/#global-results-summary","title":"Global Results Summary","text":"<p>Ranking Convergence: </p> <p>BCa Confidence Interval Convergence: </p> <p>Threshold Calculations: </p>"},{"location":"Convergence_Analysis/#discussion-convergence-modes","title":"Discussion: Convergence Modes","text":"<p>Based on these results, we can discuss the theoretical implications of the different convergence parameters one could set in HERA compared to the default settings:</p> <ul> <li>Relaxed: Prioritizes speed. While significantly faster, it carries a higher risk of terminating before true stability is reached, especially in complex scenarios and BCa confidence intervals.</li> <li>Strict: Prioritizes guaranteed stability. It enforces rigorous checks that may lead to very high iteration counts. While safer, this can be computationally expensive and potentially overkill for well-behaved datasets where the Default mode would suffice. Also it could lead to non-convergence in some cases for the desired tolerance.</li> <li>Default: Designed to balance efficiency and reliability, it seems to be the best option for general usage. Interestingly, in some tested scenarios it provides higher accuracy for the BCa confidence intervals compared to stricter settings when comparing to the reference values. This is likely because stricter settings with more iterations can occasionally include rare extreme bootstrap samples that increase variance without improving the central estimate.</li> </ul>"},{"location":"Convergence_Analysis/#conclusion","title":"Conclusion","text":"<p>Our analysis confirms that the Default settings provided with HERA should be a safe choice for general usage, achieving convergence in 99.7% of tested Ranking cases and 100% of other cases, while maintaining high accuracy against reference standards.</p>"},{"location":"Convergence_Analysis/#computational-note","title":"Computational Note","text":"<p>The complete analysis (7 scenarios \u00d7 50 independent simulations \u00d7 6 different candidates ranked per scenario, comprising 2,100 distinct datasets) was performed on a standard consumer laptop (Base Model Apple 16\" 2021 M1 MBP, 16 GB RAM) in approximately 15 hours. For each simulation, high-precision reference values were independently derived (up to B=30,000 for BCa), followed by convergence tests for all three modes (Relaxed, Default, Strict) utilizing 10\u201340 stability trials per B-step. This demonstrates the computational efficiency of the parallelized MATLAB implementation.</p>"},{"location":"Convergence_Analysis/#full-report","title":"Full Report","text":"<p>For a comprehensive look at the data, including specific breakdowns for each distribution type, please refer to the generated PDF report:</p> <p>Download Full Combined Report (PDF)</p>"},{"location":"Convergence_Analysis/#running-your-own-analysis","title":"Running Your Own Analysis","text":"<p>You can perform this analysis yourself to verify the robustness of your own changes or to test different parameter configurations. The analysis is integrated directly into the <code>start_ranking</code> command.</p>"},{"location":"Convergence_Analysis/#syntax","title":"Syntax","text":"<pre><code>HERA.start_ranking('convergence', 'true', [options])\n</code></pre>"},{"location":"Convergence_Analysis/#options","title":"Options","text":"<ul> <li><code>'logPath'</code>: (Optional) Specifies where the results and reports should be saved.</li> <li><code>'interactive'</code>: Opens a folder selection dialog.</li> <li><code>'path/to/folder'</code>: Uses the specified path.</li> <li><code>''</code> (default): Automatically saves to <code>Documents/HERA_convergence_Log</code>.</li> <li><code>'sims'</code>: (Optional) Number of simulations per scenario (Default: 15). Higher values (e.g., 50-100) provide more robust statistics but take longer to run.</li> </ul>"},{"location":"Convergence_Analysis/#examples","title":"Examples","text":"<p>1. Run with Default Settings: This runs the standard analysis (15 sims/scenario) and saves to the default Documents folder.</p> <pre><code>HERA.start_ranking('convergence', 'true')\n</code></pre> <p>2. Select Output Folder Interactively: This prompts you to choose where to save the reports.</p> <pre><code>HERA.start_ranking('convergence', 'true', 'logPath', 'interactive')\n</code></pre> <p>3. Run a High-Precision Study: Run 50 simulations per scenario for higher statistical power.</p> <pre><code>HERA.start_ranking('convergence', 'true', 'sims', 50)\n</code></pre>"},{"location":"Convergence_Modes/","title":"Convergence Modes","text":"<ul> <li>Simple Convergence: Used when <code>smoothing_window</code> is empty. Checks if the     value changes less than <code>convergence_tolerance</code> between steps.</li> <li>Robust Convergence: Used when <code>smoothing_window</code> is set (default). Uses     a moving average to smooth fluctuations and requires     <code>convergence_streak_needed</code> consecutive stable steps.</li> <li>Fallback Convergence (Elbow Method): Triggered if the primary     plateau criterion is not met. It uses a heuristic to detect the point of     diminishing returns. Warning: This is a diagnostic aid, not a guarantee!     Results should be treated as a suggestion to be verified by visual inspection     and for further analysis (for more details see Troubleshooting Convergence).</li> </ul> <p>Note: The different parameters used in the Robust Mode for convergence checking have been validated in a empirical Convergence Analysis. The default parameters were found to be sufficient for the majority of scenarios. However it is not a guarantee! Convergence failures using the elbow method were rare (0.3% of cases, exclusively in Ranking). Simple Convergence Mode was not assessed in this analysis! For detailed results, see Convergence Analysis.</p>"},{"location":"Convergence_Modes/#troubleshooting-convergence","title":"Troubleshooting Convergence","text":"<p>While the automated check should work for most datasets, \"difficult\" data with high variance or flat likelihood landscapes may fail to converge within <code>B_end</code>. In this case, you can try the following:</p> <ol> <li> <p>Check the Elbow: Inspect the generated stability plots. If you see a     clear \"elbow\" where the curve flattens but fluctuates slightly above the     strict tolerance \u03b5, the convergence parameters might be too strict     for your data's noise level.</p> </li> <li> <p>Adjust Parameters: You can relax <code>convergence_tolerance</code> (e.g., to     0.02) or increase <code>n_trials</code> and/ or <code>smoothing_window</code> in the configuration.</p> </li> <li> <p>Use Simple Convergence: Select Simple Convergence in the CLI or set     <code>smoothing_window</code> to empty to use simple convergence. Be aware that     this might not be the most robust option! Choose a higher <code>min_steps_for_convergence_check</code>     e.g. 3 to ensure that the convergence check will not be influenced by high     initial fluctuations of stability measures.</p> </li> <li> <p>Manual Override: If no clear convergence is found (no elbow), or for     theoretical guarantees of large numbers, you can just     use fixed high B values (e.g., <code>manual_B_ci = 15000</code>, <code>manual_B_thr = 2000</code>)     as per literature recommendations.</p> </li> <li> <p>Too Early Convergence: If the automated check finds convergence too     early (e.g., in robust mode), you can make the parameters stricter (e.g.,     decrease <code>convergence_tolerance</code> or increase <code>B_step</code> and/or <code>B_start</code>).</p> </li> </ol> <p>Reproducibility Note: Visual inspection of convergence plots is strongly recommended for final reporting! All procedures should use the same fixed random seed for full reproducibility.</p>"},{"location":"Example_Analysis/","title":"HERA Example Data","text":"<p>For this example we use synthetic datasets that are based on characteristics of real MRI Images. The original datasets are not included in this repository and results with these will be published in a future paper. As an example for quality metrics to objectively evaluate image quality, we use Optical Contrast (OC), Signal-to-Noise Ratio (SNR) and Contrast-to-Noise Ratio (CNR). These metrics and the way they were calculated and prioritized for the HERA analysis are based on the concept and results of Noeth et al., NMR Biomed. 2015; 28: 818-830.</p> <p>Note</p> <p>The example results and interpretations shown below serve as a didactic walkthrough to demonstrate HERA\u2019s decision-making capabilities. Please be aware that running the examples with different seeds may yield slight variations\u2014for instance, in the precise bounds of confidence intervals or in tie-breaking outcomes\u2014which can lead to a different overall ranking for some methods compared to the static screenshots. These differences reflect the inherent stochastic variability of bootstrap resampling.</p>"},{"location":"Example_Analysis/#example-use-case-1","title":"Example use case 1","text":"<p>For this example, we introduce a new imaging technique, Method D, designed to enhance the visualization of a specific Region of Interest (ROI) of up to 100 percent. We retrospectively compare Method D against five established imaging methods. Image quality metrics were automatically extracted and stored in CSV files. Our goal is to identify the optimal imaging methods for visual quantification in a subsequent study. Manually evaluating all 6 methods across 30 patients would be prohibitively time-consuming. Furthermore, if Method D is not identified as the superior method, we would need to adjust its parameters and repeat the analysis for all methods. Therefore, we employ HERA to select the top three imaging methods for the final study.</p> <p>We prioritize Optical Contrast (OC) as the primary metric, given its strong correlation with visual image quality ratings. Contrast-to-Noise Ratio (CNR) is used to compensate for lower OC, as high contrast is ineffective if noise levels are excessive. Signal-to-Noise Ratio (SNR) is prioritized last; while a lower SNR is acceptable if contrast is sufficient, a significant reduction in SNR can compromise overall image quality. Since these metrics have different scales, direct comparison or weighted prioritization is challenging. HERA helps us navigate these trade-offs to determine the best possible image quality for our specific application.</p>"},{"location":"Example_Analysis/#interpreting-the-results-1","title":"Interpreting the results 1","text":"<p>The whole analysis took about 4 Minutes to complete on my Laptop (Base Model Apple 16\" 2021 M1 MBP, 16 GB RAM). The final report PDF provides guidance on interpreting the results. Detailed Statistics are provided in the Ranking Report PDF. In this example, Method D is ranked first, followed by Method B and Method A. However, Method A exhibits a 95% confidence interval ranging from rank 3 to rank 6, whereas Method F is ranked fourth with a confidence interval between rank 2 and rank 4.</p> <p>The Sankey Diagram reveals that Method A's ranking is largely driven by its high SNR. The rank distribution under resampling conditions indicates that Method F frequently achieves a higher rank than Method A. Analysis of the Win-Loss Matrix and the CSV log file suggests that Method A outranks Method F partly due to a win in SNR against Method E, albeit with only 50% power.</p> <p>Sensitivity Analysis shows that both Method A and Method F have comparable Borda Scores. Crucially, our new technique, Method D, achieves the highest Borda Score, indicating that its top ranking is robust and not solely dependent on our specific metric prioritization. Method B's lower score is primarily due to permutations where SNR is prioritized over OC and CNR. Consequently, for our final analysis, we would select Method D, as it demonstrates the highest stable rank and the highest Borda Score, along with Method B and Method F.</p>"},{"location":"Example_Analysis/#example-results-1","title":"Example Results 1","text":"<p>Below are visual outputs generated from the Example_1 dataset using HERA.</p>"},{"location":"Example_Analysis/#final-ranking-1","title":"Final Ranking 1","text":"<p>The final ranking of the methods based on the hierarchical-compensatory logic with the confidence interval.</p> <p></p>"},{"location":"Example_Analysis/#distribution-of-bootstrap-ranks-1","title":"Distribution of Bootstrap Ranks 1","text":"<p>This diagram displays the frequency of each rank achieved by the methods across the bootstrap samples, illustrating the uncertainty in the ranking.</p> <p></p>"},{"location":"Example_Analysis/#rank-shifts-sankey-diagram-1","title":"Rank Shifts (Sankey Diagram) 1","text":"<p>This diagram shows how the ranking of methods changes across different metrics and their hierarchy level.</p> <p></p>"},{"location":"Example_Analysis/#win-loss-matrix-1","title":"Win-Loss Matrix 1","text":"<p>This matrix visualizes the pairwise comparisons between methods. Green indicates a win, red a loss, and gray neutral due to the Threshold criteria not being satisfied. The color intensity indicates the power of each comparison.</p> <p></p>"},{"location":"Example_Analysis/#example-use-case-2","title":"Example use case 2","text":"<p>We began evaluating the initial images from Method D, Method F, and Method B. Although Method D appears to offer the best overall image quality, we observed that its contrast enhancement occasionally leads to an over-representation of the Region of Interest (ROI). Conversely, the Contrast and Image Quality of Method B seem to depict the ROI size more accurately. Consequently, we sought to determine the optimal setting where Method D remains superior to Method B without excessive contrast enhancement that could compromise SNR as well.</p> <p>To achieve this, we used the same metrics as in Example 1 but introduced a new method, Method G. In Method G, we iteratively reduced the contrast enhancement calculated for Method D until it was just robustly superior to Method B. We retained all original methods, including Method D, in the analysis for comparison. Since we had already established the required number of bootstrap iterations for our data characteristics, we repeated the analysis with a fixed iteration count, reducing the HERA runtime from 5 minutes to 1 minute for each analysis step on my M1 MBP.</p>"},{"location":"Example_Analysis/#interpreting-the-results-2","title":"Interpreting the results 2","text":"<p>We progressively reduced the contrast enhancement of Method G to 20%. Below this threshold, Method G begins to lose its robust superiority over Method B, and at 15%, its confidence interval overlaps with that of Method B.</p> <p>Examining the final ranking for Example 2, we see that the addition of Method G does not fundamentally alter the initial ranking. Method G achieves a rank of 2 within its 95% confidence interval and secures the second-highest Borda Score. Based on these findings, we can proceed with recalculating our image enhancement technique using 20% contrast enhancement for the final analysis.</p>"},{"location":"Example_Analysis/#example-results-2","title":"Example Results 2","text":"<p>Below are visual outputs generated from the Example_2 dataset using HERA.</p>"},{"location":"Example_Analysis/#stability-analysis-for-example-2","title":"Stability Analysis for Example 2","text":"<p>This curve visualizes the stability of the ranking results while decreasing the contrast enhancement of Method G.</p> <p></p>"},{"location":"Example_Analysis/#final-ranking-2","title":"Final Ranking 2","text":"<p>The final ranking of the methods based on the hierarchical-compensatory logic with the confidence interval.</p> <p></p>"},{"location":"Example_Analysis/#distribution-of-bootstrap-ranks-2","title":"Distribution of Bootstrap Ranks 2","text":"<p>This diagram displays the frequency of each rank achieved by the methods across the bootstrap samples, illustrating the uncertainty in the ranking.</p> <p></p>"},{"location":"Example_Analysis/#rank-shifts-sankey-diagram-2","title":"Rank Shifts (Sankey Diagram) 2","text":"<p>This diagram shows how the ranking of methods changes across different metrics and their hierarchy level.</p> <p></p>"},{"location":"Example_Analysis/#win-loss-matrix-2","title":"Win-Loss Matrix 2","text":"<p>This matrix visualizes the pairwise comparisons between methods. Green indicates a win, red a loss, and gray neutral due to the Threshold criteria not being satisfied. The color intensity indicates the power of each comparison.</p> <p></p>"},{"location":"Input_Data_Specification/","title":"Input Data Specification","text":"<ul> <li>Format: CSV or Excel (<code>.xlsx</code>).</li> <li>Organization: One file per metric.</li> <li>Filename: The filename (excluding extension) must strictly match the     corresponding entry in <code>metric_names</code> (e.g., <code>Accuracy.csv</code> for metric     <code>Accuracy</code>).</li> <li>Dimensions: Rows = Observations (n, e.g., Subjects), Columns =     Datasets (N, e.g., Methods).</li> <li>Consistency: All files must have identical dimensions. Uneven sample sizes     (missing data) are handled by automatic <code>NaN</code> padding (empty cells) to     ensure a uniform matrix size, and pairwise deletion is applied during     analysis.</li> </ul>"},{"location":"Methodological_Guidelines_%26_Limitations/","title":"Methodological Guidelines &amp; Limitations","text":"<p>The statistical rigor of HERA (e.g., Holm-Bonferroni correction, Bootstrapping) imposes practical limits on the number of datasets (N) and sample size (n). Therefore the following guidelines are provided as theoretical considerations but should not be taken as strict requirements.</p>"},{"location":"Methodological_Guidelines_%26_Limitations/#number-of-datasets-n","title":"Number of Datasets (N)","text":"<p>Increasing N quadratically increases the number of pairwise comparisons (m = N(N-1)/2), which reduces statistical power due to strict corrections.</p> <ul> <li>Minimum (N = 3): Required for a meaningful ranking. (N = 2 is just a     simple comparison).</li> <li>Optimal (N \u2248 8\u201310): Balances ranking depth with statistical     power (28\u201345 comparisons).</li> <li>Upper Limit (N \u2248 15): Not generally recommended. The loss of power     from FWER corrections makes detecting true differences unlikely. However,     it is possible to use HERA with N &gt; 15 and you can just give it a try.</li> </ul> <p>Visual Limit (N \u2264 20): While HERA should technically compute rankings for any N (exported to CSV/JSON), the generated plots (e.g. Win-Loss Matrix, Final Summary) visually degrade beyond N = 20. For N &gt; 20, I recommend relying on the machine-readable and text-based outputs. You can disable plots (<code>create_reports: false</code>) to save runtime.</p> <p>Recommendation: If you have a large pool of candidates (N &gt;&gt; 15), it could be a good idea to apply a global screening method (e.g., Friedman Test followed by Nemenyi post-hoc) to identify the top tier of algorithms. Ranking the entire set with HERA may be overly strict; instead, select the top performing subset (e.g., the best 10-15) and use HERA for the final ranking of the best candidates.</p>"},{"location":"Methodological_Guidelines_%26_Limitations/#sample-size-n","title":"Sample Size (n)","text":"<p>A balance between statistical stability and computational feasibility is required.</p> <ul> <li>Minimum (n = 16): Required for the Wilcoxon test to use the Normal     Approximation in Matlab.</li> <li>Robust Min (n \u2248 25\u201330): Necessary for stable BCa confidence     intervals and Jackknife estimates (Although it automatically switches     to Percentil Bootstrap if Bias or Jackknife estimates become unstable).</li> <li>Optimal (n \u2248 50\u2013300): Best balance of power, stability, and     runtime.</li> <li>Upper Limit (n \u2248 1,000\u20135,000): Higher n improves statistics     but linearly scales runtime. n \u226b 5,000 may be computationally     impractical due to extensive bootstrapping.</li> </ul> <p>Recommendation: Perform an a priori power analysis to estimate the required n for your chosen N.</p>"},{"location":"Methodological_Guidelines_%26_Limitations/#missing-data-handling-nan","title":"Missing Data Handling (NaN)","text":"<p>HERA is robust against missing data (<code>NaN</code>) but handling it comes with trade-offs:</p> <ul> <li>Pairwise Deletion: HERA employs pairwise deletion to maximize data     usage without requiring complex imputation. While this assumes data is missing     completely at random (MCAR), it remains methodologically robust: By relying     on discrete, independent pairwise comparisons, the algorithm avoids the     mathematical inconsistencies (e.g., non-positive definite matrices) that     typically compromise pairwise deletion in global multivariate statistics.</li> <li>Robust Bootstrapping: If <code>NaN</code>s are detected, HERA automatically switches     to a \"Robust Path\". This dynamically filters invalid data for each bootstrap     resample to ensure correctness, which significantly increases runtime     especially for large sample sizes (n).</li> <li>Automatic Warning: A warning is issued if valid data drops below 80% for     any comparison however it is not a strict requirement.</li> </ul> <p>Recommendation: Minimize <code>NaN</code>s to preserve statistical power and performance. For critical analyses with substantial data loss, use validated imputation methods (e.g., MICE) before running HERA.</p>"},{"location":"Methodological_Guidelines_%26_Limitations/#number-of-metrics-m-3","title":"Number of Metrics (M \u2264 3)","text":"<p>HERA is designed for a maximum of 3 hierarchical metrics to maintain methodological robustness. This limit is inherent to the hierarchical-compensatory design and is based on the following methodological considerations:</p> <ul> <li>Loss of Interpretability: With every additional hierarchical level, the causal     chain of the ranking decision becomes opaque and increasingly difficult to trace.     Limiting the depth to 3 levels ensures that the ranking logic remains transparent     and empirically justifiable.</li> <li>Increased Risk of Collinearity: Adding more metrics increases the probability     of introducing redundant criteria (e.g., two metrics measuring valid features     of the same underlying property). In a sequential logic, these correlates     would be falsely treated as independent evidence, distorting the ranking.</li> <li>Functional Saturation: The hierarchical-compensatory logic is fully saturated     by three levels (Sorting, Correction, Finalization). Adding a fourth metric     yields diminishing margins of utility, as the probability of meaningful rank     adjustments approaches zero, while the complexity of the decision model     increases disproportionately.</li> </ul> <p>Recommendation: If you want to consider more than 3 metrics and use HERA you could first perform a check for collinearity (e.g., using a correlation matrix). Strongly correlated metrics could be aggregated into a common factor (e.g., via Principal Component Analysis (PCA)) before running the HERA analysis with up to 3 metrics.</p> <p>If your study design requires the simultaneous integration of a large number of metrics (\\(M \\gg 3\\)) HERA is not feasible and compensatory or outranking MCDA methods are methodologically more appropriate. In this case, approaches like TOPSIS or PROMETHEE might be a better choice.</p>"},{"location":"Methodology/","title":"Theoretical Background","text":""},{"location":"Methodology/#summary","title":"Summary","text":"<p>In scientific disciplines ranging from clinical research to machine learning, researchers face the challenge of objectively comparing multiple algorithms, experimental conditions, or datasets across a variety of performance metrics. This process, often framed as Multi-Criteria Decision Making (MCDM), is critical for identifying state-of-the-art methods. However, traditional ranking approaches frequently suffer from limitations: they may rely on central tendencies that ignore data variability (Benavoli et al., 2016; Dem\u0161ar, 2006), depend solely on p-values which can be misleading in large samples (Wasserstein &amp; Lazar, 2016), or require subjective weighting of conflicting metrics (Taherdoost &amp; Madanchian, 2023).</p> <p>HERA (Hierarchical-Compensatory, Effect-Size Driven Ranking Algorithm) is a MATLAB toolbox designed to automate this comparison process, bridging the gap between elementary statistical tests and complex decision-making frameworks. Unlike weighted-sum approaches that collapse multi-dimensional performance into a single scalar, HERA implements a hierarchical-compensatory logic. This logic integrates non-parametric significance testing (Wilcoxon signed-rank test), robust effect size estimation (Cliff\u2019s Delta, Relative Difference), and bootstrapping (e.g.\u00a0Percentile and Cluster) to produce rankings that are both statistically robust and practically relevant. HERA is designed for researchers in biomedical imaging, machine learning, and applied statistics who need to compare method performance across multiple quality metrics in a statistically rigorous manner without requiring subjective parameter tuning.</p>"},{"location":"Methodology/#statement-of-need","title":"Statement of Need","text":"<p>The scientific community increasingly recognizes the pitfalls of relying on simple summary statistics or p-values alone (Wasserstein &amp; Lazar, 2016). In benchmarking studies, specifically, several issues persist:</p> <ol> <li>Ignoring Variance: Ranking based on mean scores fails to account for the stability of performance across different subjects or folds. A method might achieve a high average score due to exceptional performance on a few easy cases while failing catastrophically on others, yet still outrank a more consistent competitor.</li> <li>Statistical vs.\u00a0Practical Significance: A result can be statistically significant but practically irrelevant, especially in large datasets where even trivial differences yield p\u2004&lt;\u20040.05. Standard tests do not inherently distinguish between these cases, potentially leading to the adoption of methods that offer no tangible benefit (Sullivan &amp; Feinn, 2012).</li> <li>Subjectivity in Aggregation: Many MCDM methods require users to assign subjective weights to metrics (e.g., \u201cAccuracy is 0.7, Speed is 0.3\u201d). These weights are often chosen post-hoc or lack empirical justification, introducing researcher bias that can be manipulated to favor a specific outcome (Taherdoost &amp; Madanchian, 2023).</li> <li>Distributional Assumptions: Parametric tests (e.g., t-test) assume normality, which is often violated in real-world benchmarks where performance metrics may be skewed, bounded, or ordinal (Romano et al., 2006).</li> </ol> <p>HERA addresses these challenges by providing a standardized, data-driven framework. It ensures that a method is only ranked higher if it demonstrates a statistically significant and sufficiently large advantage, preventing \u201cwins\u201d based on negligible differences or noise. Unlike existing MCDM software packages such as the Python libraries pyDecision (Pereira et al., 2024) and pymcdm (Kizielewicz et al., 2023), or R\u2019s RMCDA (Najafi &amp; Mirzaei, 2025), which often implement classical methods like TOPSIS (Hwang &amp; Yoon, 1981), PROMETHEE (Brans &amp; Vincke, 1985), and ELECTRE (Roy, 1968) that require user-defined weights or preference functions, HERA eliminates subjective parameterization by using data-driven thresholds derived from bootstrap resampling. Furthermore, HERA integrates statistical hypothesis testing directly into the ranking process, a feature absent in standard MCDM toolboxes. While the MATLAB ecosystem offers robust statistical functions, it currently lacks a dedicated, open-source toolbox that unifies this advanced MCDM method with bootstrap validation, forcing researchers to rely on ad-hoc scripts.</p>"},{"location":"Methodology/#methodological-framework","title":"Methodological Framework","text":"<p>HERA operates on paired data matrices where rows represent subjects (or datasets) and columns represent the methods to be compared. The core innovation is its sequential logic, which allows for \u201ccompensation\u201d between metrics based on strict statistical evidence.</p>"},{"location":"Methodology/#statistical-rigor-and-effect-sizes","title":"Statistical Rigor and Effect Sizes","text":"<p>HERA quantifies differences using statistical significance and effect sizes to ensure practical relevance independent of sample size (Cohen, 1988; Sullivan &amp; Feinn, 2012). A \u201cwin\u201d always requires satisfying three conjunctive criteria, if not it is considered \u201cneutral\u201d:</p> <ul> <li>Significance: p\u2004&lt;\u2004\u03b1<sub>Holm</sub> (Holm-Bonferroni corrected). Pairwise comparisons use the Wilcoxon signed-rank test (Wilcoxon, 1945), with p-values corrected using the step-down Holm-Bonferroni method (Holm, 1979) to control the Family-Wise Error Rate (FWER).</li> <li>Stochastic Dominance (Cliff\u2019s Delta): Cliff\u2019s Delta (d\u2004=\u2004P(X&gt;Y)\u2005\u2212\u2005P(Y&gt;X)) quantifies distribution overlap, is robust to outliers, and relates to common-language effect sizes (Cliff, 1993; Vargha &amp; Delaney, 2000). The effect size d must exceed a bootstrapped threshold \u03b8<sub>d</sub>.</li> <li>Magnitude (Relative Difference): The Relative Difference (RelDiff) quantifies effect magnitude on the original metric scale, normalized by the mean absolute value. This normalization is formally identical to the Symmetric Mean Absolute Percentage Error (SMAPE) used in forecasting (Makridakis, 1993) and conceptually related to the Response Ratio, which uses logarithmic ratios to compare effects across studies (Hedges et al., 1999). The metric enables scale-independent comparisons and facilitates the interpretation of percentage changes (Kampenes et al., 2007). RelDiff must exceed a threshold \u03b8<sub>r</sub>.</li> </ul> <p>Dual Criteria &amp; SEM Lower Bound HERA\u2019s complementary logic requires both dominance and magnitude, preventing \u201cwins\u201d based on trivial consistent differences or noisy outliers (Lakens, 2013). Thresholds are determined via Percentile Bootstrapping (lower \u03b1/2-quantile) (Rousselet et al., 2021). To filter noise in low-variance datasets, the RelDiff threshold enforces a lower bound based on the Standard Error of the Mean (SEM), ensuring \u03b8<sub>r</sub>\u2004\u2265\u2004\u03b8<sub>SEM</sub>. This approach is inspired by the concept of the \u201cSmallest Worthwhile Change\u201d (Hopkins, 2004), but adapted for HERA to quantify the uncertainty of the group mean rather than individual measurement error.</p>"},{"location":"Methodology/#hierarchical-compensatory-logic","title":"Hierarchical-Compensatory Logic","text":"<p>The ranking process is structured as a multi-stage tournament. It does not use a global score but refines the rank order iteratively (see Fig. 1):</p> Hierarchical-Compensatory Ranking Logic <ul> <li>Stage 1 (Initial Sort): Methods are initially ranked based on the win count of the primary metric M<sub>1</sub>. In case of a tie in adjacent ranks, Cliff\u2019s Delta is used to break the tie. If Cliff\u2019s Delta is zero, the raw mean values are used to break the tie.</li> <li>Stage 2 (Compensatory Correction): This stage addresses the trade-off between metrics. A lower-ranked method can \u201cswap\u201d places with a higher-ranked method if it shows a statistically significant and relevant superiority in a secondary metric M<sub>2</sub>. This effectively implements a lexicographic ordering with a compensatory component (Keeney &amp; Raiffa, 1976), allowing a method that is slightly worse in the primary metric but vastly superior in a secondary metric to improve its standing.</li> <li>Stage 3 (Tie-Breaking): This stage resolves \u201cneutral\u201d results using a tertiary metric M<sub>3</sub>. It applies two sub-logics to ensure a total ordering: <ul> <li>Sublogic 3a: A one-time correction if the previous metric is \u201cneutral\u201d based on the HERA criteria. This handles cases where two methods are indistinguishable in the second metric while still respecting the initial ranking.</li> <li>Sublogic 3b: To resolve groups of remaining undecided methods, an iterative correction loop is applied if both M<sub>1</sub> and M<sub>2</sub> are \u201cneutral\u201d, iteratively using metric M<sub>3</sub> until a final stable ranking is found.</li> </ul></li> </ul>"},{"location":"Methodology/#validation-and-uncertainty","title":"Validation and Uncertainty","text":"<p>HERA integrates advanced resampling methods to quantify uncertainty:</p> <ul> <li>BCa Confidence Intervals: Bias-Corrected and Accelerated (BCa) intervals are calculated for all effect sizes (DiCiccio &amp; Efron, 1996).</li> <li>Cluster Bootstrap: To assess the stability of the final ranking, HERA performs a cluster bootstrap resampling subjects with replacement (Field &amp; Welsh, 2007). This yields a 95% confidence interval for the rank of each method.</li> <li>Power Analysis: A post-hoc simulation with bootstrap estimates the probability of detecting a \u201cwin\u201d, \u201closs\u201d or \u201cneutral\u201d in all tested metrics given the data characteristics.</li> <li>Sensitivity Analysis: The algorithm permutes the metric hierarchy and aggregates the resulting rankings using a Borda Count (Young, 1974) to evaluate the robustness of the decision against hierarchy changes.</li> </ul>"},{"location":"Methodology/#software-features","title":"Software Features","text":"<p>HERA offers a flexible configuration of up to three metrics (see Fig. 2). This allows users to adapt the ranking logic to different study designs and needs. It also provides a range of reporting options, data integration, and reproducibility features.</p> <ul> <li>Automated Reporting: Generates PDF reports, Win-Loss Matrices, Sankey Diagrams, and machine-readable JSON/CSV exports.</li> <li>Reproducibility: Supports fixed-seed execution and configuration file-based workflows. The full analysis state, including random seeds and parameter settings, is saved in a JSON file, allowing other researchers to exactly replicate the ranking results.</li> <li>Convergence Analysis: To avoid the common pitfall of using an arbitrary number of bootstrap iterations, HERA implements an adaptive algorithm. It automatically monitors the stability of the estimated confidence intervals and effect size thresholds, continuing the resampling process until the estimates converge within a specified tolerance, thus determining the optimal number of iterations B dynamically (Pattengale et al., 2010). If the characteristics of the data for bootstrapping are known, the number of bootstrap iterations can be set manually.</li> <li>Data Integration: HERA supports seamless data import from standard formats (CSV, Excel) and MATLAB tables, facilitating integration into existing research pipelines. Example datasets and workflows demonstrating practical applications are included in the repository.</li> <li>Accessibility: HERA can be easily installed by cloning the GitHub repository and running a setup script, or deployed as a standalone application that requires no MATLAB license. An interactive command-line interface guides users through the analysis without requiring programming expertise, while an API and JSON Configuration allow for automated batch processing.</li> </ul> Flexible Configuration options for Ranking Logic"},{"location":"Methodology/#acknowledgements","title":"Acknowledgements","text":"<p>This software was developed at the Institute of Neuroradiology, Goethe University Frankfurt. I thank Prof.\u00a0Dr.\u00a0Dipl.-Phys. Ralf Deichmann (Cooperative Brain Imaging Center, Goethe University Frankfurt) for his support during the initial conceptualization of this project. I acknowledge Dr.\u00a0med. Christophe Arendt (Institute of Neuroradiology, Goethe University Frankfurt) for his supervision and support throughout the project. I also thank Rejane Golbach PhD (Institute of Biostatistics and Mathematical Modeling, Goethe University Frankfurt) for her valuable feedback on the statistical methodology.</p>"},{"location":"Methodology/#references","title":"References","text":"Benavoli, A., Corani, G., &amp; Mangili, F. (2016). Should we really use post-hoc tests based on mean-ranks? Journal of Machine Learning Research, 17, 1\u201310. https://jmlr.org/papers/v17/benavoli16a.html  Brans, J. P., &amp; Vincke, P. (1985). A preference ranking organization method (the PROMETHEE method for multiple criteria decision-making). Management Science, 31(6), 647\u2013656. https://doi.org/10.1287/mnsc.31.6.647  Cliff, N. (1993). Dominance statistics: Ordinal analyses to answer ordinal questions. Psychological Bulletin, 114(3), 494\u2013509. https://doi.org/10.1037/0033-2909.114.3.494  Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum Associates. https://doi.org/10.4324/9780203771587  Dem\u0161ar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7, 1\u201330. https://jmlr.org/papers/v7/demsar06a.html  DiCiccio, T. J., &amp; Efron, B. (1996). Bootstrap confidence intervals. Statistical Science, 11(3), 189\u2013228. https://doi.org/10.1214/ss/1032280214  Field, C. A., &amp; Welsh, A. H. (2007). Bootstrapping clustered data. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(3), 369\u2013390. https://doi.org/10.1111/j.1467-9868.2007.00593.x  Hedges, L. V., Gurevitch, J., &amp; Curtis, P. S. (1999). The meta-analysis of response ratios in experimental ecology. Ecology, 80(4), 1150\u20131156. https://doi.org/10.1890/0012-9658(1999)080[1150:TMAORR]2.0.CO;2  Holm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6(2), 65\u201370. https://www.jstor.org/stable/4615733  Hopkins, W. G. (2004). How to interpret changes in an athletic performance test. Sportscience, 8, 1\u20137. https://www.sportsci.org/jour/04/wghtests.htm  Hwang, C. L., &amp; Yoon, K. (1981). Multiple attribute decision making: Methods and applications. Springer. https://doi.org/10.1007/978-3-642-48318-9  Kampenes, V. B., Dyb\u00e5, T., Hannay, J. E., &amp; Sj\u00f8berg, D. I. K. (2007). A systematic review of effect size in software engineering experiments. Information and Software Technology, 49(11\u201312), 1073\u20131086. https://doi.org/10.1016/j.infsof.2007.02.015  Keeney, R. L., &amp; Raiffa, H. (1976). Decisions with multiple objectives: Preferences and value trade-offs. Wiley. https://doi.org/10.1017/CBO9781139174084  Kizielewicz, B., Shekhovtsov, A., &amp; Salabun, W. (2023). Pymcdm\u2014the universal library for solving multi-criteria decision-making problems. SoftwareX, 22, 101368. https://doi.org/10.1016/j.softx.2023.101368  Lakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative science: A practical primer for t-tests and ANOVAs. Frontiers in Psychology, 4, 863. https://doi.org/10.3389/fpsyg.2013.00863  Makridakis, S. (1993). Accuracy measures: Theoretical and practical concerns. International Journal of Forecasting, 9(4), 527\u2013529. https://doi.org/10.1016/0169-2070(93)90079-3  Najafi, A., &amp; Mirzaei, S. (2025). RMCDA: The comprehensive r library for applying multi-criteria decision analysis methods. Software Impacts, 24, 100762. https://doi.org/10.1016/j.simpa.2025.100762  Pattengale, N. D., Alipour, M., Bininda-Emonds, O. R. P., Moret, B. M. E., &amp; Stamatakis, A. (2010). How many bootstrap replicates are necessary? Journal of Computational Biology, 17(3), 337\u2013354. https://doi.org/10.1089/cmb.2009.0179  Pereira, V., Basilio, M. P., &amp; Santos, C. H. T. (2024). Enhancing decision analysis with a large language model: pyDecision a comprehensive library of MCDA methods in python. Journal of Modelling in Management. https://doi.org/10.1108/JM2-04-2024-0118  Romano, J., Kromrey, J. D., Coraggio, J., &amp; Skowronek, J. (2006). Appropriate statistics for ordinal level data: Should we really be using t-test and cohen\u2019s d for evaluating group differences on the NSSE and other surveys? Proceedings of the Annual Meeting of the Florida Association of Institutional Research. https://www.researchgate.net/publication/237544991  Rousselet, G. A., Pernet, C. R., &amp; Wilcox, R. R. (2021). The percentile bootstrap: A primer with step-by-step instructions in R. Advances in Methods and Practices in Psychological Science, 4(1), 1\u201310. https://doi.org/10.1177/2515245920911881  Roy, B. (1968). Classement et choix en pr\u00e9sence de points de vue multiples (la m\u00e9thode ELECTRE). Revue Fran\u00e7aise d\u2019informatique Et de Recherche Op\u00e9rationnelle, 2(V1), 57\u201375. https://doi.org/10.1051/ro/196802V100571  Sullivan, G. M., &amp; Feinn, R. (2012). Using effect size\u2014or why the p value is not enough. Journal of Graduate Medical Education, 4(3), 279\u2013282. https://doi.org/10.4300/JGME-D-12-00156.1  Taherdoost, H., &amp; Madanchian, M. (2023). Multi-criteria decision making (MCDM) methods and concepts. Encyclopedia, 3(1), 235\u2013250. https://doi.org/10.3390/encyclopedia3010006  Vargha, A., &amp; Delaney, H. D. (2000). A critique and improvement of the CL common language effect size statistics of McGraw and wong. Journal of Educational and Behavioral Statistics, 25(2), 101\u2013132. https://doi.org/10.3102/10769986025002101  Wasserstein, R. L., &amp; Lazar, N. A. (2016). The ASA statement on p-values: Context, process, and purpose. The American Statistician, 70(2), 129\u2013133. https://doi.org/10.1080/00031305.2016.1154108  Wilcoxon, F. (1945). Individual comparisons by ranking methods. Biometrics Bulletin, 1(6), 80\u201383. https://doi.org/10.2307/3001968  Young, H. P. (1974). An axiomatization of borda\u2019s rule. Journal of Economic Theory, 9(1), 43\u201352. https://doi.org/10.1016/0022-0531(74)90073-8"},{"location":"Python_Integration/","title":"Python Integration","text":"<p>HERA provides a compiled Python interface that allows seamless integration into Python-based data science pipelines. This package wraps the MATLAB functions and provides them as native Python objects.</p> <p>Note: The package utilizes the MATLAB Runtime. It must be installed separately as described below.</p>"},{"location":"Python_Integration/#1-installation-for-end-users","title":"1. Installation (For End Users)","text":"<p>The easiest way to install the package is via <code>pip</code> from PyPI.</p>"},{"location":"Python_Integration/#step-1-install-package","title":"Step 1: Install Package","text":"<pre><code>pip3 install hera-matlab\n</code></pre>"},{"location":"Python_Integration/#step-2-install-matlab-runtime","title":"Step 2: Install MATLAB Runtime","text":"<p>HERA requires the MATLAB Runtime R2025b (v25.2). After installing the package, run the following command to check if you have the correct runtime installed or to get the direct download link:</p> <pre><code>python3 -m hera_matlab.install_runtime\n</code></pre> <p>Follow the instructions provided by this command to download and install the runtime if it is missing.</p>"},{"location":"Python_Integration/#step-3-macos-specifics-critical","title":"Step 3: macOS Specifics (Critical)","text":"<p>On macOS, you cannot use the standard <code>python</code> interpreter to import the package, as it will likely fail with a library loading error. Instead, you must use the <code>mwpython</code> wrapper provided by the MATLAB Runtime.</p> <p>Location of mwpython: Typically found at: <code>/Applications/MATLAB/MATLAB_Runtime/R2025b/bin/mwpython</code></p> <p>Usage: Instead of running <code>python3 script.py</code>, run:</p> <pre><code>/Applications/MATLAB/MATLAB_Runtime/R2025b/bin/mwpython script.py\n</code></pre> <p>You may want to add this to your PATH variable for easier access.</p> <p>Warning</p> <p>Python Version Compatibility: HERA (via MATLAB Runtime) currently supports Python 3.9, 3.10, 3.11, and 3.12. Python 3.13 is NOT supported.</p> <p>If you have Python 3.13 installed, <code>mwpython</code> will fail to load your packages. Solution: Create and activate a Virtual Environment with Python 3.12 before running <code>mwpython</code>.</p> <pre><code># Example using brew to install python 3.12\nbrew install python@3.12\n/opt/homebrew/bin/python3.12 -m venv .venv_hera\nsource .venv_hera/bin/activate\npip3 install hera-matlab\n# Now mwpython will automatically use this environment\nmwpython script.py\n</code></pre>"},{"location":"Python_Integration/#2-usage-modes","title":"2. Usage Modes","text":""},{"location":"Python_Integration/#a-standard-pipeline-file-based","title":"A. Standard Pipeline (File-Based)","text":"<p>This mode replicates the MATLAB batch processing workflow. It runs the complete analysis based on a JSON configuration file and automatically generates all PDF reports and plots on disk.</p> <p>Note: The interactive command-line interface (CLI) is not supported in the Python package. You must use a configuration file.</p> <pre><code>import hera_matlab\n\n# Initialize Runtime\nhera = hera_matlab.initialize()\n\n# Run with JSON configuration\n# Outputs (PDFs, Images) will be saved to the 'output_dir' defined in the config\nhera.start_ranking('configFile', 'analysis_config.json', nargout=0)\n\nhera.terminate()\n</code></pre>"},{"location":"Python_Integration/#b-direct-data-integration-numpypandas","title":"B. Direct Data Integration (NumPy/Pandas)","text":"<p>This mode allows you to use HERA as a computational engine within your Python scripts (e.g., Jupyter Notebooks). You can pass data directly from NumPy/Pandas and receive the ranking results as a Python dictionary, enabling seamless integration into larger data science pipelines.</p> <pre><code>import hera_matlab\nimport matlab\n\n# Initialize\nhera = hera_matlab.initialize()\n\n# Prepare Data (Convert NumPy arrays to MATLAB types)\n# Example: 2 Subjects x 2 Methods\ndata_m1 = matlab.double([[0.1, 0.5], [0.2, 0.4]])\ndata_m2 = matlab.double([[1.0, 3.0], [1.2, 2.9]])\n\n# Configure Analysis\nconfig = {\n    'custom_data': [data_m1, data_m2],\n    'metric_names': ['Runtime', 'Accuracy'],\n    'dataset_names': ['Method A', 'Method B'],\n    'ranking_mode': 'M1_M2',\n    'output_dir': 'my_hera_results' # Optional: Specify output folder\n}\n\n# Execute Ranking and retrieve Dictionary\nresults = hera.run_ranking(config, nargout=1)\n\n# Access Results\nprint(f\"Final Ranks: {results['final_rank']}\")\nprint(f\"Effect Sizes (Cliff's Delta): {results['d_vals_all']}\")\nprint(f\"Effect Sizes (Rel Diff): {results['rel_vals_all']}\")\nprint(f\"P-Values: {results['p_vals_all']}\")\n\nhera.terminate()\n</code></pre> <p>See Results Structure Reference for a complete list of available fields</p>"},{"location":"Python_Integration/#c-automatic-data-conversion-v120","title":"C. Automatic Data Conversion (v1.2.0+)","text":"<p>Starting with version 1.2.0, the package includes a smart wrapper that performs bidirectional conversion:</p> <ol> <li>Input: Automatically converts NumPy arrays and Pandas DataFrames to the required MATLAB types.</li> <li>Output: Automatically converts returned MATLAB data (e.g., <code>matlab.double</code>) back into NumPy arrays for easy processing in Python.</li> </ol> <p>This eliminates the need for manual conversion in both directions.</p> <pre><code>import hera_matlab\nimport numpy as np\n\n# Initialize\nhera = hera_matlab.initialize()\n\n# Option 1: Using NumPy Arrays\ndata_m1 = np.array([[0.1, 0.5], [0.2, 0.4]])\ndata_m2 = np.array([[1.0, 3.0], [1.2, 2.9]])\n\n# (Option 2: Pandas DataFrames are also supported)\n\nconfig = {\n    'custom_data': [data_m1, data_m2],\n    'metric_names': ['Runtime', 'Accuracy'],\n    'dataset_names': ['Method A', 'Method B'],\n    'ranking_mode': 'M1_M2',\n    'output_dir': 'hera_numpy_results'\n}\n\n# The wrapper automatically converts the data before calling MATLAB\nresults = hera.run_ranking(config, nargout=1)\n\nprint(f\"Final Ranks: {results['final_rank']}\")\n</code></pre> <p>Note: Explicit termination (<code>hera.terminate()</code>) is not required in scripts, as the package handles cleanup automatically.</p>"},{"location":"Python_Integration/#3-build-instructions-for-maintainers","title":"3. Build Instructions (For Maintainers)","text":"<p>To generate the installer and Python package from source (requires MATLAB Compiler SDK):</p> <ol> <li>Run the Build Helper:</li> </ol> <pre><code>./deploy/build_and_prep_pypi.sh\n</code></pre> <p>This script compiles the MATLAB code, injects the runtime checks, and prepares    the distribution artifacts (<code>.whl</code>, <code>.tar.gz</code>) in <code>deploy/dist</code>.</p> <ol> <li>Distribution:    Upload the generated artifacts from <code>deploy/dist</code> to PyPI or GitHub Releases.</li> </ol>"},{"location":"Python_Integration/#4-running-the-test-suite","title":"4. Running the Test Suite","text":"<p>You can execute the internal HERA verification and test suite (Scientific, System, and Unit tests) directly from Python to ensure the installation is valid and mathematically correct. For more details on the test suite, see the Testing section in the main documentation.</p> <pre><code>import hera_matlab\n\n# Initialize\nhera = hera_matlab.initialize()\n\n# Launch Test Mode\n# Arguments: 'runtest', 'true'\n# This will execute all tests and print the results to standard output/log files.\nhera.start_ranking('runtest', 'true', nargout=0)\n\nhera.terminate()\n</code></pre> <p>This is useful for verifying deployments on new machines (e.g., CI/CD).</p>"},{"location":"Python_Integration/#5-running-convergence-analysis","title":"5. Running Convergence Analysis","text":"<p>You can also trigger the robust convergence analysis directly from Python. This is identical to the MATLAB <code>HERA.start_ranking('convergence', 'true')</code> command. For more details on the analysis and its parameters, see Convergence Analysis.</p> <pre><code>import hera_matlab\n\n# Initialize\nhera = hera_matlab.initialize()\n\n# Run Convergence Analysis\n# Arguments: 'convergence', 'true'\n# Optional: 'sims', 50 (for higher precision)\n# Optional: 'logPath', '/path/to/log' (or 'interactive')\nhera.start_ranking('convergence', 'true', nargout=0)\n\nhera.terminate()\n</code></pre>"},{"location":"Ranking_Modes_Explained/","title":"Ranking Modes Explained","text":"Mode Behavior Use Case <code>M1</code> Ranks strictly by Metric 1. Single-metric evaluation. <code>M1_M2</code> Metric 1 is primary. Metric 2 can correct (swap) ranks if a lower-ranked method significantly outperforms a higher-ranked one in Metric 2. Balancing performance vs. Accuracy. <code>M1_M3A</code> Metric 1 is primary. Metric 2 acts strictly as a tie-breaker. Tie-breaking without overriding primary results. <code>M1_M2_M3</code> Full hierarchy. M1 is primary. M2 corrects M1 (iterative). M3 applies two sub-logics: (1) One-time correction if M2 is neutral, and (2) Iterative tie-breaking if both M1 and M2 are neutral. Complex multi-objective benchmarking."},{"location":"Repository_Structure/","title":"Repository Structure","text":"<p>The codebase is organized as a MATLAB package (<code>+HERA</code>) to ensure namespace isolation.</p> <pre><code>HERA-Matlab/\n\u251c\u2500\u2500 +HERA/                         % Main Package Namespace\n\u2502   \u251c\u2500\u2500 +output/                   % Report Generation (PDF, JSON, CSV)\n\u2502   \u251c\u2500\u2500 +analysis/                 % Scientific Analysis Modules\n\u2502   \u2502   \u2514\u2500\u2500 convergence_analysis.m % Robust Convergence Verification\n\u2502   \u251c\u2500\u2500 +plot/                     % Visualization (Sankey, Heatmaps)\n\u2502   \u251c\u2500\u2500 +run/                      % Execution Logic\n\u2502   \u251c\u2500\u2500 +start/                    % CLI &amp; Configuration Logic\n\u2502   \u251c\u2500\u2500 +stats/                    % Statistical Core (Cliff's Delta, Convergence Check)\n\u2502   \u251c\u2500\u2500 +test/                     % Unit Test Suite\n\u2502   \u251c\u2500\u2500 language/                  % Localization Files\n\u2502   \u251c\u2500\u2500 bootstrap_ranking.m        % Cluster Bootstrap Analysis\n\u2502   \u251c\u2500\u2500 borda_ranking.m            % Consensus Ranking\n\u2502   \u251c\u2500\u2500 calculate_bca_ci.m         % BCa Confidence Intervals\n\u2502   \u251c\u2500\u2500 calculate_ranking.m        % Core Ranking Logic\n\u2502   \u251c\u2500\u2500 calculate_thresholds.m     % Threshold Calculation\n\u2502   \u251c\u2500\u2500 Contents.m                 % Toolbox Documentation (ver, help)\n\u2502   \u251c\u2500\u2500 default.m                  % Global Defaults\n\u2502   \u251c\u2500\u2500 design.m                   % Style &amp; Design Definitions\n\u2502   \u251c\u2500\u2500 generate_output.m          % Output Generation Controller\n\u2502   \u251c\u2500\u2500 generate_plots.m           % Plot Generation Controller\n\u2502   \u251c\u2500\u2500 get_language.m             % Language Loader\n\u2502   \u251c\u2500\u2500 get_version.m              % Version Retrieval\n\u2502   \u251c\u2500\u2500 load_data.m                % Data Import &amp; Validation\n\u2502   \u251c\u2500\u2500 power_analysis.m           % Power Analysis\n\u2502   \u251c\u2500\u2500 run_ranking.m              % Core Function (Developer API)\n\u2502   \u251c\u2500\u2500 start_ranking.m            % Main Entry Point (User API)\n\u2502   \u2514\u2500\u2500 run_unit_test.m            % Test Runner\n\u251c\u2500\u2500 assets/                        % Images &amp; Logos\n\u251c\u2500\u2500 data/                          % Data Directory\n\u2502   \u251c\u2500\u2500 examples/                  % Synthetic Example Datasets\n\u2502   \u2514\u2500\u2500 README.md                  % Data Documentation\n\u251c\u2500\u2500 deploy/                        % Build Scripts (Standalone App)\n\u251c\u2500\u2500 docs/                          % Documentation &amp; Guides\n\u251c\u2500\u2500 paper/                         % Paper Resources\n\u251c\u2500\u2500 tests/                         % Unit Test &amp; Analysis Reports\n\u251c\u2500\u2500 setup_HERA.m                   % Path Setup Script\n\u251c\u2500\u2500 CITATION.cff                   % Citation Metadata\n\u251c\u2500\u2500 CODE_OF_CONDUCT.md             % Community Standards\n\u251c\u2500\u2500 CONTRIBUTING.md                % Contribution Guidelines\n\u251c\u2500\u2500 LICENSE                        % License File\n\u2514\u2500\u2500 README.md                      % Global Documentation\n</code></pre>"},{"location":"Results_Structure_Reference/","title":"Results Structure Reference","text":"<p>When running <code>results = HERA.run_ranking(...)</code>, the returned structure contains:</p> Field Dimensions Description Final Results <code>final_rank</code> <code>[N x 1]</code> Final rank for each dataset (1 = Best). <code>final_order</code> <code>[1 x N]</code> Indices of datasets sorted by rank. <code>final_bootstrap_ranks</code> <code>[N x B]</code> Bootstrapped rank distribution for stability analysis. <code>ci_lower_rank</code> <code>[N x 1]</code> Lower bound of Rank Confidence Interval. <code>ci_upper_rank</code> <code>[N x 1]</code> Upper bound of Rank Confidence Interval. <code>thresholds</code> struct Thresholds for Cliff's Delta and RelDiff. Statistics <code>d_vals_all</code> <code>[Pairs x M]</code> Cliff's Delta effect sizes for all pairs/metrics. <code>rel_vals_all</code> <code>[Pairs x M]</code> Relative Mean Differences. <code>ci_d_all</code> <code>[Pairs x 2 x M]</code> BCa Confidence Intervals for Delta. <code>ci_r_all</code> <code>[Pairs x 2 x M]</code> BCa Confidence Intervals for RelDiff. <code>all_p_value_matrices</code> <code>{1 x M}</code> Cell array of raw p-values (Wilcoxon). <code>all_alpha_matrices</code> <code>{1 x M}</code> Cell array of Holm-Bonferroni corrected alphas. <code>all_sig_matrices</code> <code>{1 x M}</code> Logical matrices indicating significant wins. Diagnostics <code>swap_details</code> struct Log of logic-based rank swaps (M1 vs M2). <code>intermediate_orders</code> struct Rankings after each hierarchical stage. <code>borda_results</code> struct Consensus ranking from sensitivity analysis. <code>power_results</code> struct Post-hoc power analysis data. <code>all_permutation_ranks</code> <code>[N x Perms]</code> Ranks for every metric permutation tested. <code>selected_permutations</code> <code>[Perms x M]</code> Indices of metrics for each permutation."},{"location":"Standalone_Runtime/","title":"Standalone Runtime","text":"<p>HERA can be compiled into a standalone application for macOS, Linux, and Windows. The build process generates an installer that automatically downloads the required MATLAB Runtime, making it easy to distribute.</p> <p>Download: A pre-built installer for macOS (Apple Silicon) is available as a ZIP archive in the Releases section.</p>"},{"location":"Standalone_Runtime/#building-the-installer","title":"Building the Installer","text":"<p>To build the installer, you need a MATLAB installation with the MATLAB Compiler toolbox.</p> <ol> <li>Open MATLAB and navigate to the project root.</li> <li>Run the build script:</li> </ol> <pre><code>cd deploy\nbuild_HERA_matlab\n</code></pre> <ol> <li>The artifacts (Installer + ZIP) will be generated in <code>deploy/output/matlab</code>.</li> </ol>"},{"location":"Standalone_Runtime/#2-installation","title":"2. Installation","text":"<p>The generated installer handles the dependency setup for you.</p> <ol> <li>Run the Installer:</li> <li>General: Download and extract the ZIP archive from the release.</li> <li>Windows: Double-click <code>HERA_Runtime_Installer.exe</code>.</li> <li>macOS: Double-click <code>HERA_Runtime_Installer.app</code>.</li> <li>Linux: Run the installer executable from the terminal.</li> <li>Finish Installation: Follow the on-screen prompts. The installer will automatically download and install the correct MATLAB Runtime if it's missing on your system.</li> </ol>"},{"location":"Standalone_Runtime/#3-usage","title":"3. Usage","text":""},{"location":"Standalone_Runtime/#gui-launcher-interactive","title":"GUI Launcher (Interactive)","text":"<p>To start the application in the standard interactive mode:</p> <ul> <li>Windows: Launch <code>HERA_Runtime</code> from the installation directory.</li> <li>macOS: Double-click the <code>HERA_Launcher.command</code> script provided with the release.</li> <li>Linux: Run <code>./run_HERA_Runtime.sh &lt;RuntimePath&gt;</code> from the terminal.</li> </ul>"},{"location":"Standalone_Runtime/#command-line-interface-macos","title":"Command Line Interface (macOS)","text":"<p>For advanced usage, you can run the application directly from the terminal using the <code>HERA_Launcher.command</code> script. This allows you to pass arguments for batch processing, testing, or analysis.</p> <p>Note: Ensure you are in the directory containing the launcher script.</p>"},{"location":"Standalone_Runtime/#1-batch-processing-non-interactive","title":"1. Batch Processing (Non-Interactive)","text":"<p>Run a full ranking analysis using a configuration file, skipping the UI. For more details on configuration parameters, see Configuration &amp; Parameters.</p> <pre><code>./HERA_Launcher.command configFile \"/absolute/path/to/config.json\"\n</code></pre>"},{"location":"Standalone_Runtime/#2-run-unit-tests","title":"2. Run Unit Tests","text":"<p>Execute the internal test suite to verify the integrity of the installation. For more details on the test suite, see the Testing section in the main documentation.</p> <pre><code>./HERA_Launcher.command runtest true\n</code></pre>"},{"location":"Standalone_Runtime/#3-run-convergence-analysis","title":"3. Run Convergence Analysis","text":"<p>Perform the robust convergence verification study. For more details, refer to the Convergence Analysis Documentation.</p> <pre><code>./HERA_Launcher.command convergence true\n</code></pre>"},{"location":"landing_header/","title":"HERA: Official Documentation","text":"<p>Welcome to the official documentation for the HERA (Hierarchical-Compensatory, Effect-Size-Driven and Non-Parametric Ranking Algorithm) MATLAB toolbox.</p> <p>This platform serves as the comprehensive reference guide for the HERA framework, systematically organizing the repository's documentation into a centralized, wiki-style knowledge hub.</p> <p>While the source code contains the raw files, this site is optimized to offer a streamlined reading experience for the entire framework\u2014including detailed installation guides, parameter configuration, hierarchical-compensatory ranking logic, and bootstrap validation mechanisms.</p>"},{"location":"landing_header/#get-hera-v121","title":"Get HERA (v1.2.1)","text":"<p>You can download the latest version of the toolbox directly:</p> <ul> <li>GitHub Releases: Download .mltbx or Source</li> <li>MATLAB File Exchange: Download Toolbox</li> </ul>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#hera-hierarchical-compensatory-effect-size-driven-and-non-parametric-ranking-algorithm","title":"HERA: Hierarchical-Compensatory, Effect-Size-Driven and Non-Parametric Ranking Algorithm","text":"<p>Made for Scientific Benchmarking</p> <p>Key Features \u2022 Installation \u2022 Quick Start \u2022 Documentation \u2022 Citation</p>"},{"location":"overview/#overview","title":"Overview","text":"<p>HERA is a MATLAB toolbox designed to automate the objective comparison of algorithms, experimental conditions, or datasets across multiple quality metrics. Unlike traditional ranking methods that rely solely on mean values or p-values, HERA employs a hierarchical-compensatory logic that integrates:</p> <ul> <li>Significance Testing: Wilcoxon signed-rank tests for paired data.</li> <li>Effect Sizes: Cliff's Delta and Relative Mean Difference for practical relevance.</li> <li>Bootstrapping: Data-driven thresholds and BCa confidence intervals.</li> </ul> <p>This ensures that a \"win\" is only counted if it is both statistically significant and practically relevant, providing a robust and nuanced ranking system.</p>"},{"location":"overview/#key-features","title":"Key Features","text":"<ul> <li>Hierarchical Logic: Define primary and secondary metrics. Secondary   metrics can act as tie-breakers or rank correctors (e.g., <code>M1_M2</code>,   <code>M1_M2_M3</code>).</li> <li>Data-Driven Thresholds: Automatically calculates adaptive effect size   thresholds using Percentile Bootstrapping.</li> <li>Robustness: Utilizes Bias-Corrected and Accelerated (BCa) confidence   intervals and Cluster Bootstrapping for rank stability.</li> <li>Automated Reporting: Generates PDF reports, Win-Loss Matrices, Sankey   Diagrams, and machine-readable JSON/CSV exports.</li> <li>Reproducibility: Supports fixed-seed execution and configuration   file-based workflows.</li> </ul>"},{"location":"overview/#installation","title":"Installation","text":""},{"location":"overview/#requirements","title":"Requirements","text":"<ul> <li>MATLAB (R2020a or later Required)</li> <li>Statistics and Machine Learning Toolbox (Required)</li> <li>Parallel Computing Toolbox (Required for performance)</li> </ul>"},{"location":"overview/#setup","title":"Setup","text":""},{"location":"overview/#option-a-matlab-toolbox-recommended","title":"Option A: MATLAB Toolbox (Recommended)","text":"<ol> <li>Download the latest <code>HERA_vX.Y.Z.mltbx</code> from the    Releases page.</li> <li>Double-click the file to install it.</li> <li>Done! HERA is now available as a command (<code>HERA.start_ranking</code>) in MATLAB.</li> </ol>"},{"location":"overview/#option-b-git-clone-for-developers","title":"Option B: Git Clone (for Developers)","text":"<ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/lerdmann1601/HERA-Matlab.git\n</code></pre> </li> <li> <p>Install/Configure Path:</p> <p>Navigate to the repository folder and run the setup script to add HERA to your MATLAB path.</p> <pre><code>cd HERA-Matlab\nsetup_HERA\n</code></pre> </li> </ol> <p>\ud83d\udc49 Standalone Runtime</p> <p>\ud83d\udc49 Python Integration</p> <p>\ud83d\udc49 Automated Build (GitHub Actions)</p>"},{"location":"overview/#quick-start","title":"Quick Start","text":""},{"location":"overview/#1-interactive-mode-recommended-for-beginners","title":"1. Interactive Mode (Recommended for Beginners)","text":"<p>The interactive command-line interface guides you through every step of the configuration, from data selection to statistical parameters. If you are new to HERA, this is the recommended mode. At any point, you can exit the interface by typing <code>exit</code> or <code>quit</code> or <code>q</code>.</p> <pre><code>HERA.start_ranking()\n</code></pre>"},{"location":"overview/#2-batch-mode-reproducible-server","title":"2. Batch Mode (Reproducible / Server)","text":"<p>For automated analysis or reproducible research, use a JSON configuration file. For more details on configuration parameters, see Configuration &amp; Parameters.</p> <pre><code>HERA.start_ranking('configFile', 'config.json')\n</code></pre>"},{"location":"overview/#3-unit-test-mode","title":"3. Unit Test Mode","text":"<p>Run the built-in validation suite to ensure HERA is working correctly on your system. For more details, see the Testing section.</p> <pre><code>% Run tests and save log to default location\nHERA.start_ranking('runtest', 'true')\n\n% Run tests and save log to a specific folder\nHERA.start_ranking('runtest', 'true', 'logPath', '/path/to/logs')\n</code></pre>"},{"location":"overview/#4-convergence-analysis","title":"4. Convergence Analysis","text":"<p>Perform a robust scientific validation of the default convergence parameters. For more details, see Convergence Analysis.</p> <pre><code>% Run analysis and save log to default location\nHERA.start_ranking('convergence', 'true')\n\n% Run analysis and save log to a specific folder\nHERA.start_ranking('convergence', 'true', 'logPath', '/path/to/logs')\n</code></pre> <p>Note: Example use cases with synthetic datasets and results are provided in the <code>data/examples</code> directory. See Example Analysis for a walkthrough of the example use cases and visual examples of the ranking outputs.</p> <p>Note: HERA is designed for high-performance scientific computing, featuring fully parallelized bootstrap procedures and automatic memory management to optimize efficiency. However, specifically due to the extensive use of bootstrapping, it remains a CPU-intensive application. Please ensure you have access to enough CPU cores for reasonable performance.</p>"},{"location":"overview/#documentation","title":"Documentation","text":"<p>\ud83d\udc49 Repository Structure</p> <p>\ud83d\udc49 Theoretical Background</p> <p>\ud83d\udc49 Ranking Modes Explained</p> <p>\ud83d\udc49 Input Data Specification</p> <p>\ud83d\udc49 Example Analysis</p> <p>\ud83d\udc49 Methodological Guidelines &amp; Limitations</p> <p>\ud83d\udc49 Configuration &amp; Parameters</p> <p>\ud83d\udc49 Bootstrap Configuration</p> <p>\ud83d\udc49 Convergence Modes</p> <p>\ud83d\udc49 Convergence Analysis</p> <p>\ud83d\udc49 Advanced Usage (Developer Mode)</p> <p>\ud83d\udc49 Results Structure Reference</p>"},{"location":"overview/#outputs","title":"Outputs","text":"<p>HERA generates a timestamped directory containing:</p> <pre><code>Ranking_&lt;Timestamp&gt;/\n\u251c\u2500\u2500 Output/\n\u2502   \u251c\u2500\u2500 results_*.csv                 % Final ranking table (Mean \u00b1 SD of metrics and rank CI)\n\u2502   \u251c\u2500\u2500 data_*.json                   % Complete analysis record (Inputs, Config, Stats, Results)\n\u2502   \u251c\u2500\u2500 log_*.csv                     % Detailed log of pairwise comparisons and logic\n\u2502   \u251c\u2500\u2500 sensitivity_details_*.csv     % Results of the Borda sensitivity analysis\n\u2502   \u251c\u2500\u2500 BCa_Correction_Factors_*.csv  % Correction factors (Bias/Skewness) for BCa CIs\n\u2502   \u2514\u2500\u2500 bootstrap_rank_*.csv          % Complete distribution of bootstrapped ranks\n\u251c\u2500\u2500 Graphics/                         % High-res PNGs organized in subfolders\n\u2502   \u251c\u2500\u2500 Ranking/\n\u2502   \u251c\u2500\u2500 Detail_Comparison/\n\u2502   \u251c\u2500\u2500 CI_Histograms/\n\u2502   \u2514\u2500\u2500 Threshold_Analysis/\n\u251c\u2500\u2500 PDF/                              % Specialized reports\n\u2502   \u251c\u2500\u2500 Ranking_Report.pdf\n\u2502   \u251c\u2500\u2500 Convergence_Report.pdf\n\u2502   \u2514\u2500\u2500 Bootstrap_Report.pdf\n\u251c\u2500\u2500 Final_Ranking_*.png               % Summary graphic of ranking result\n\u251c\u2500\u2500 Final_Report_*.pdf                % Consolidated graphical report of the main results\n\u251c\u2500\u2500 Ranking_*.txt                     % Complete console log of the session\n\u2514\u2500\u2500 configuration.json                % Reusable configuration file to reproduce the run\n</code></pre>"},{"location":"overview/#testing","title":"Testing","text":"<p>HERA includes a comprehensive validation framework (<code>run_unit_test.m</code>) comprising 46 test cases organized into four suites:</p> <ol> <li>Unit Tests (19 cases): Checks individual components, helper functions, and     execution logic (Run/Start packages) to ensure specific parts of the code work     correctly.</li> <li>Statistical Tests (5 cases): Verifies the core mathematical functions     (e.g., Jackknife, Cliff's Delta) and ensures the performance optimizations     (hybrid switching) work as intended.</li> <li>Scientific Tests (19 cases): Comprehensive validation of ranking logic,     statistical accuracy, and robustness against edge cases (e.g., zero     variance, outliers).</li> <li>System Tests (3 cases): Runs the entire HERA pipeline from start to     finish to ensure that the JSON configuration (batch), Developer API and NaN     Data handling are working correctly.</li> </ol>"},{"location":"overview/#running-tests","title":"Running Tests","text":"<p>You can run the test suite in three ways:</p> <ol> <li> <p>Auto-Log Mode (Default)     Automatically finds a writable folder (e.g., Documents) to save the log     file.</p> <pre><code>HERA.run_unit_test()\n</code></pre> </li> <li> <p>Interactive Mode     Opens a dialog to select where to save the log file.</p> <pre><code>HERA.run_unit_test('interactive')\n</code></pre> </li> <li> <p>Custom Path Mode     Saves the log file to a specific directory.</p> <pre><code>HERA.run_unit_test('/path/to/my/logs')\n</code></pre> </li> </ol>"},{"location":"overview/#github-actions-cloud-testing","title":"GitHub Actions (Cloud Testing)","text":"<p>For reviewers or users without a local MATLAB license, you can run the test suite directly on GitHub:</p> <ol> <li>Go to the Actions tab in this repository.</li> <li>Select Testing HERA from the left sidebar.</li> <li>Click Run workflow.</li> </ol>"},{"location":"overview/#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see CONTRIBUTING.md for details.</p> <ol> <li>Fork the repository.</li> <li>Create a feature branch.</li> <li>Commit your changes.</li> <li>Open a Pull Request.</li> </ol>"},{"location":"overview/#citation","title":"Citation","text":"<p>If you use HERA in your research, please cite:</p> <pre><code>@software{HERA_Matlab,\n  author = {von Erdmannsdorff, Lukas},\n  title = {HERA: A Hierarchical-Compensatory, Effect-Size Driven and Non-parametric\n  Ranking Algorithm using Data-Driven Thresholds and Bootstrap Validation},\n  url = {https://github.com/lerdmann1601/HERA-Matlab},\n  version = {1.2.1},\n  doi = {10.5281/zenodo.18274870},\n  year = {2026}\n}\n</code></pre>"},{"location":"overview/#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"}]}